{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586a3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ChromeOptions\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException , ElementNotInteractableException ,StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from newspaper import Article\n",
    "import os\n",
    "\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caee627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service('M:/chromedriver-win64/chromedriver-win64/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service= service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4a416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file where infos are saved\n",
    "file = 'papers.csv'\n",
    "with open(file , 'w',encoding='utf-8') as f :\n",
    "    header =  ['Title','Author','Abstract','Submission_date','URL']\n",
    "    writer = csv.writer(f , delimiter = ';')\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c44b31",
   "metadata": {},
   "source": [
    "# Scrapping arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "53772148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivScrapper():\n",
    "    \n",
    "    def __init__(self,keyword):\n",
    "        self.research_key = keyword\n",
    "        self.month_list = ['january','february','march','april',\n",
    "                           'may','june','july','august','september',\n",
    "                           'october','november','december']\n",
    "        \n",
    "        self.url = 'https://arxiv.org/'\n",
    "        self.file = 'papers.csv'\n",
    "        \n",
    "    def make_search(self):\n",
    "        driver.get(self.url)\n",
    "        search_bar = driver.find_element(By.NAME , 'query')\n",
    "        subject = self.research_key\n",
    "        search_bar.send_keys(subject)\n",
    "        search_bar.send_keys(Keys.ENTER)\n",
    "        \n",
    "    def scrape_an_offer(self,paper):\n",
    "        title = paper.find_element(By.CLASS_NAME , 'title').text\n",
    "        authors = paper.find_element(By.CLASS_NAME , 'authors').text\n",
    "        \n",
    "        try : \n",
    "            more_button = paper.find_element(By.CLASS_NAME ,'is-size-7')\n",
    "            more_button.click()\n",
    "        except Exception :\n",
    "            pass\n",
    "\n",
    "        abstract = paper.find_element(By.CLASS_NAME ,'abstract-full').text\n",
    "\n",
    "        submission_date = paper.find_element(By.CSS_SELECTOR , 'p.is-size-7').text.split(';')[0]\n",
    "        submission_date = self.clean_date(submission_date)\n",
    "\n",
    "        url_to_paper = paper.find_element(By.CSS_SELECTOR , 'p.list-title a').get_attribute('href')\n",
    "        paper_infos = [title, authors, abstract, submission_date, url_to_paper]\n",
    "        \n",
    "        return paper_infos\n",
    "    \n",
    "    def scrape_all_offers(self):\n",
    "        \n",
    "        self.make_search()\n",
    "        i = 0\n",
    "        \n",
    "        while True :\n",
    "            paper_cards = driver.find_elements(By.CLASS_NAME , 'arxiv-result')\n",
    "            for paper in tqdm(paper_cards):\n",
    "                paper_infos = self.scrape_an_offer(paper)\n",
    "                self.save_offer(paper_infos)\n",
    "\n",
    "            try :\n",
    "                if i<= 10 :\n",
    "                    next_page_button = driver.find_element(By.CLASS_NAME ,'pagination-next')\n",
    "                    next_page_button.click()\n",
    "                    time.sleep(5) \n",
    "                    i += 1\n",
    "                else :\n",
    "                    break\n",
    "                    \n",
    "            except NoSuchElementException:\n",
    "                    break\n",
    "                \n",
    "    \n",
    "    def clean_date(self,date, source = 'arxiv'):\n",
    "        date = re.sub('Submitted |\\,','',date)\n",
    "        day, month, year = date.split()\n",
    "        \n",
    "        month_idx = self.month_list.index(month.lower()) + 1\n",
    "        date = datetime.date(int(year),month_idx,int(day) )\n",
    "        return date\n",
    "    \n",
    "    def save_offer(self ,infos):\n",
    "        with open(self.file , 'a',encoding='utf-8') as f :\n",
    "            writer = csv.writer(f , delimiter = ';')\n",
    "            writer.writerow(infos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "47273be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069dc28b41994769a45b0f75c2eb2eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ce4412afb64d50a18fc795cf4beab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985ca33e1c994b098aa021aa2bdfabb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25963c9c5b4f4be3ab81a794930943d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ff910ace1d4ab1ab090c4ca4aa5e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbcbef17b1542d8b17e4b3f57a432b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5844941d1c4849adb5e0e8baa0c5b7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26aeb19f389410fa0bac00c0acd19a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecebbe457244ca8a689e95253f5f51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981cc9f04b704ad7be2fcdfbacdf8ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4048895fff649f0ba7d1178de24686d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc81c5301a941f9b8c684316c955863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scraper = ArxivScrapper('computer vision')\n",
    "scraper.scrape_all_offers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482492b",
   "metadata": {},
   "source": [
    "# Scraping Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ae6797a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumScrapper():\n",
    "    def __init__(self):\n",
    "        self.month_list = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "        self.url = 'https://medium.com/nlplanet'\n",
    "        self.paper_file = 'papers.csv'\n",
    "        self.guide_file = 'guides.csv'\n",
    "        self.news_file = 'news.csv'\n",
    "        self.author = 'Fabio Chiusano'\n",
    "        \n",
    "    def make_search(self):\n",
    "        driver.get(self.url)\n",
    "        \n",
    "    def scrape_one_news(self,new):\n",
    "        new_title = new.find_element(By.CSS_SELECTOR ,'a.af').text\n",
    "        details = new.text\n",
    "        url_to_source = new.find_element(By.CSS_SELECTOR ,'a.af').get_attribute('href')\n",
    "\n",
    "        try :\n",
    "             date = driver.find_element(By.XPATH ,'//span[@data-testid = \"storyPublishDate\"]').text\n",
    "        except NoSuchElementException :\n",
    "            date = driver.find_element(By.CSS_SELECTOR ,'span.be div.ab.ae').text.split('\\n')[-1]\n",
    "\n",
    "        date = self.clean_date_medium(date)\n",
    "        news_infos = [new_title , self.author , details ,date ,url_to_source ]\n",
    "        \n",
    "        return news_infos\n",
    "    \n",
    "    def scrape_one_guide(self,guide) :\n",
    "        guide_title = guide.find_element(By.CSS_SELECTOR ,'a.af').text\n",
    "        guide_details = guide.text\n",
    "         \n",
    "        try :\n",
    "             date = driver.find_element(By.XPATH ,'//span[@data-testid = \"storyPublishDate\"]').text\n",
    "        except NoSuchElementException :\n",
    "            date = driver.find_element(By.CSS_SELECTOR ,'span.be div.ab.ae').text.split('\\n')[-1]\n",
    "            \n",
    "        url_to_source = guide.find_element(By.CSS_SELECTOR ,'a.af').get_attribute('href')\n",
    "        guide_infos = [guide_title , self.author , guide_details ,date ,url_to_source ]\n",
    "        \n",
    "        return guide_infos\n",
    "    \n",
    "    def scrape_one_article(self, paper):\n",
    "        paper_title = paper.find_element(By.CSS_SELECTOR ,'a.af').text\n",
    "        paper_details = paper.text\n",
    "        url_to_paper = paper.find_element(By.CSS_SELECTOR ,'a.af').get_attribute('href')\n",
    "        \n",
    "        try :\n",
    "             date = driver.find_element(By.XPATH ,'//span[@data-testid = \"storyPublishDate\"]').text\n",
    "        except NoSuchElementException :\n",
    "            date = driver.find_element(By.CSS_SELECTOR ,'span.be div.ab.ae').text.split('\\n')[-1]\n",
    "            \n",
    "        paper_infos = [paper_title , self.author , paper_details ,date ,url_to_paper]\n",
    "        return paper_infos\n",
    "        \n",
    "    def scrape_all_offers(self):\n",
    "        \n",
    "        self.make_search()\n",
    "        current_url = driver.current_url\n",
    "        \n",
    "        for article_number in tqdm(range(5)): \n",
    "            \n",
    "            try :\n",
    "                articles = WebDriverWait(driver,10,ignored_exceptions = StaleElementReferenceException).until(EC.presence_of_all_elements_located((By.TAG_NAME ,'article'))) \n",
    "                article = articles[article_number]\n",
    "                article.click()\n",
    "\n",
    "\n",
    "                assert driver.current_url != current_url\n",
    "                \n",
    "            except AssertionError:\n",
    "                print(f'Broken for page {article_number}')\n",
    "                continue\n",
    "\n",
    "            news , guides, papers = WebDriverWait(driver,10,ignored_exceptions = StaleElementReferenceException).until(EC.presence_of_all_elements_located((By.TAG_NAME ,'ul'))) \n",
    "            \n",
    "            # Scrapping the news\n",
    "            news_element = news.find_elements(By.TAG_NAME ,'li')\n",
    "            for new in news_element :\n",
    "                new_infos = self.scrape_one_news(new)\n",
    "                self.save_offer(self.news_file,new_infos)\n",
    "                \n",
    "            # Scrapping the guides\n",
    "            guide_elements = guides.find_elements(By.TAG_NAME ,'li')\n",
    "            for guide in guide_elements :\n",
    "                guides_info = self.scrape_one_guide(guide)\n",
    "                self.save_offer(self.guide_file,guides_info)\n",
    "                \n",
    "            # Scrapping the papers\n",
    "            paper_elements = papers.find_elements(By.TAG_NAME ,'li')\n",
    "            for paper in paper_elements :\n",
    "                paper_info = self.scrape_one_article(paper)\n",
    "                self.save_offer(self.paper_file , paper_info)\n",
    "                \n",
    "            driver.back()\n",
    "            \n",
    "                \n",
    "    \n",
    "    def clean_date_medium(self, date):\n",
    "        if date.find('hour') != -1:\n",
    "            date = datetime.date.today()\n",
    "            \n",
    "        elif date.find('ago') != -1 :\n",
    "            day = datetime.date.today().day -  int(date.split(' ')[0])\n",
    "            date = datetime.date(2024,datetime.date.today().month,day)\n",
    "        else :\n",
    "            day = date.split()[1][:-1]\n",
    "            month = self.month_list.index(date.split()[0]) + 1\n",
    "            date = datetime.date(2024 ,month,int(day))\n",
    "        return date\n",
    "    \n",
    "    def save_offer(self ,file, infos):\n",
    "        with open(file , 'a',encoding='utf-8') as f :\n",
    "            writer = csv.writer(f , delimiter = ';')\n",
    "            writer.writerow(infos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e5a98520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7956c7475a742ebbc34d4f8e275f0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broken for page 1\n"
     ]
    }
   ],
   "source": [
    "scraper = MediumScrapper()\n",
    "scraper.scrape_all_offers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fa0cb",
   "metadata": {},
   "source": [
    "# Scrapping Paper_with_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c088e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWCScrapper():  \n",
    "    def __init__(self):\n",
    "        self.month_list = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n",
    "        \n",
    "        self.url = 'https://paperswithcode.com/'\n",
    "        self.file = 'papers.csv'\n",
    "        \n",
    "    def make_search(self):\n",
    "        driver.get(self.url)\n",
    "\n",
    "        \n",
    "    def scrape_an_offer(self,paper):\n",
    "        paper_title = driver.find_element(By.TAG_NAME , 'h1').text\n",
    "        authors_span = driver.find_elements(By.CLASS_NAME,'author-span')\n",
    "\n",
    "        publish_date = authors_span[0].text\n",
    "        publish_date = self.clean_date(publish_date)\n",
    "\n",
    "        authors = ' ,'.join(author.text for author in authors_span[1:4])\n",
    "        abstract = driver.find_element(By.CSS_SELECTOR , 'div.paper-abstract p').text\n",
    "        url_to_paper = driver.find_element(By.CSS_SELECTOR , 'div.paper-abstract a').get_attribute('href')\n",
    "\n",
    "        paper_infos = [paper_title , authors , abstract ,publish_date ,url_to_paper ]\n",
    "\n",
    "        return paper_infos\n",
    "    \n",
    "    def scrape_all_offers(self):\n",
    "        \n",
    "        self.make_search()\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR,'div.item-content h1 a')\n",
    "        \n",
    "        for article in tqdm(articles) : \n",
    "            driver.execute_script(\"arguments[0].click();\", article)\n",
    "            paper_infos = self.scrape_an_offer(article)\n",
    "            self.save_offer(paper_infos)\n",
    "            driver.back()\n",
    "            time.sleep(2)\n",
    "            \n",
    "    \n",
    "    def clean_date(self,date):\n",
    "        try:\n",
    "            date = re.sub('Submitted |\\,','',date)\n",
    "            day, month, year = date.split()\n",
    "\n",
    "            month_idx = self.month_list.index(month.lower()) + 1\n",
    "            date = datetime.date(int(year),month_idx,int(day))\n",
    "        except ValueError :\n",
    "            date = datetime.date(2024,1,1)\n",
    "        return date\n",
    "    \n",
    "    def save_offer(self ,infos):\n",
    "        with open(self.file , 'a',encoding='utf-8') as f :\n",
    "            writer = csv.writer(f , delimiter = ';')\n",
    "            writer.writerow(infos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "232d59df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db2a8dd99c242409b6584e134ce2a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scraper = PWCScrapper()\n",
    "scraper.scrape_all_offers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
