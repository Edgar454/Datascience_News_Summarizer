{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException , ElementClickInterceptedException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = 'https://www.glassdoor.com/Job/morocco-jobs-SRCH_IL.0,7_IN162.htm'\n",
    "url_2 = 'https://ma.indeed.com/?from=jobsearch-empty-whatwhere'\n",
    "url_3 = 'https://www.linkedin.com/jobs/search?keywords=&location=Morocco&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "service = Service('M:/chromedriver-win64/chromedriver-win64/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = webdriver.ChromeOptions()\n",
    "#opt.add_argument('headless')\n",
    "driver = webdriver.Chrome(service= s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping url_1 : glassdor\n",
    "driver.get(url_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the search\n",
    "post = 'Data Scientist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('Jobs.csv','w') as f :\n",
    "    header = ['Job','Employer' ,'Employer Rating','Location','Description']\n",
    "    writer = csv.writer(f , delimiter =';')\n",
    "    writer.writerow(header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glassdor_scrapper :\n",
    "    \n",
    "    def __init__(self , driver = driver):\n",
    "        self.driver = driver\n",
    "    \n",
    "    def get_offers(self ,query):\n",
    "        search_bar_job = self.driver.find_element(By.ID,'searchBar-jobTitle')\n",
    "        search_bar_location = self.driver.find_element(By.ID,'searchBar-location')\n",
    "\n",
    "        search_bar_job.send_keys(query)\n",
    "        search_bar_job.send_keys(Keys.ENTER)\n",
    "        \n",
    "    def scrape_offers(self):\n",
    "        jobs = driver.find_elements(By.CLASS_NAME , 'JobCard_jobCardContainer__l0svv')\n",
    "        i = 0\n",
    "        for job in jobs :\n",
    "            try :\n",
    "                job.click()\n",
    "            except ElementClickInterceptedException :\n",
    "                wait = WebDriverWait(driver, 20)\n",
    "                wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'CloseButton'))).click()\n",
    "                \n",
    "            time.sleep(2)\n",
    "            results = self.scrape_one_offer()\n",
    "                \n",
    "            i+= 1   \n",
    "            self.save_offers(results)\n",
    "        return i\n",
    "        \n",
    "        \n",
    "    def scrape_one_offer(self) :\n",
    "        # all the details of the job\n",
    "        job_details = self.driver.find_element(By.CLASS_NAME , 'JobDetails_jobDetailsContainer__sS1W1')\n",
    "        \n",
    "        # scrapping the name of the job\n",
    "        title = job_details.find_element(By.CLASS_NAME,'JobDetails_jobTitle__Rw_gn').text\n",
    "\n",
    "        if title.lower().find('data') != -1 :\n",
    "            \n",
    "            # scrapping the employer name =\n",
    "            employer = job_details.find_element(By.CLASS_NAME , 'EmployerProfile_employerName__Xemli').text\n",
    "\n",
    "            # Some employers don't have rating\n",
    "            try : \n",
    "                employer_rating = job_details.find_element(By.CLASS_NAME , 'EmployerProfile_ratingContainer__N4hxE').text\n",
    "            except NoSuchElementException:\n",
    "                employer_rating = None\n",
    "            \n",
    "            # scrapping the location\n",
    "            location = job_details.find_element(By.CLASS_NAME , 'JobDetails_location__MbnUM').text\n",
    "\n",
    "            try :\n",
    "                more_button = job_details.find_element(By.CLASS_NAME , 'JobDetails_showMore__j5Z_h')\n",
    "                try :\n",
    "                    more_button.click()\n",
    "                except ElementClickInterceptedException :\n",
    "                    wait = WebDriverWait(driver, 20)\n",
    "                    wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'CloseButton'))).click()\n",
    "\n",
    "            except NoSuchElementException :\n",
    "                pass\n",
    "            # scrapping the description\n",
    "            job_description = job_details.find_element(By.CLASS_NAME , 'JobDetails_jobDescription__6VeBn').text\n",
    "            job_infos = [title,employer,employer_rating,location,job_description]\n",
    "            return job_infos\n",
    "    \n",
    "    def save_offers(self,infos):\n",
    "        \n",
    "        with open('Jobs.csv','a') as f :\n",
    "            writer = csv.writer(f , delimiter =';')\n",
    "            writer.writerow(job_infos)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Glassdor_scrapper(driver)\n",
    "scraper.get_offers(post)\n",
    "_ = scraper.scrape_offers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping url_2 : Indeed \n",
    "post = 'Data Scientist'\n",
    "town = 'Casablanca'\n",
    "driver.get(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indeed_scrapper :\n",
    "    \n",
    "    def __init__(self , driver = driver):\n",
    "        self.driver = driver\n",
    "    \n",
    "    def get_offers(self ,query , location) :\n",
    "        search_bar_text = self.driver.find_element(By.ID , 'text-input-what')\n",
    "        search_bar_text.send_keys(query)\n",
    "\n",
    "        search_bar_town = self.driver.find_element(By.ID , 'text-input-where')\n",
    "        search_bar_town.send_keys(location)\n",
    "\n",
    "        search_button = self.driver.find_element(By.CLASS_NAME,'yosegi-InlineWhatWhere-primaryButton')\n",
    "        search_button.click()\n",
    "        \n",
    "    def scrape_offers(self):\n",
    "        \n",
    "        while True :\n",
    "            jobs = self.driver.find_elements(By.CLASS_NAME , 'css-mk9n32')\n",
    "            i = 0\n",
    "            for job in jobs :\n",
    "                try :\n",
    "                    job.click()\n",
    "                    time.sleep(3)\n",
    "                except ElementClickInterceptedException :\n",
    "                    wait = WebDriverWait(self.driver, 20)\n",
    "                    wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@aria-label=\"fermer\"]'))).click()\n",
    "\n",
    "                results = self.scrape_one_offer()   \n",
    "                self.save_offers(results)\n",
    "                i+= 1\n",
    "            try :           \n",
    "                next_page_button = self.driver.find_element(By.XPATH , '//a[@aria-label=\"Next Page\"]')\n",
    "                next_page_button.click()\n",
    "\n",
    "            except NoSuchElementException :\n",
    "                break\n",
    "            \n",
    "        return i\n",
    "        \n",
    "        \n",
    "    def scrape_one_offer(self) :\n",
    "        \n",
    "        # all the details of the job\n",
    "        job_details = self.driver.find_element(By.CLASS_NAME , 'jobsearch-JobComponent')\n",
    "        \n",
    "        # scrape the name of the job\n",
    "        title = job_details.find_element(By.CLASS_NAME , 'jobsearch-JobInfoHeader-title').text\n",
    "\n",
    "        if title.lower().find('data') != -1 :\n",
    "            # scrape the employer name\n",
    "            try :\n",
    "                employer = job_details.find_element(By.CLASS_NAME , 'css-1saizt3').text\n",
    "            except NoSuchElementException :\n",
    "                employer = job_details.find_element(By.CLASS_NAME,'css-1cxc9zk').text\n",
    "                \n",
    "            # scrape the location\n",
    "            location = job_details.find_element(By.CLASS_NAME , 'css-1ikmi61').text\n",
    "            \n",
    "            # scrape the description\n",
    "            description = job_details.find_element(By.ID , 'jobDescriptionText').text\n",
    "            \n",
    "            # scrape the employer_rating\n",
    "            employer_rating = None\n",
    "\n",
    "            job_infos = [title,employer,employer_rating,location,job_description]\n",
    "            \n",
    "            return job_infos\n",
    "    \n",
    "    def save_offers(self,infos):\n",
    "        \n",
    "        with open('Jobs.csv','a') as f :\n",
    "            writer = csv.writer(f , delimiter =';')\n",
    "            writer.writerow(job_infos)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Indeed_scrapper(driver)\n",
    "scraper.get_offers(post,town)\n",
    "_ = scraper.scrape_offers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping url_3 : LinKedIn\n",
    "driver.get(url_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinKedIn_scrapper :\n",
    "    \n",
    "    def __init__(self , driver = driver):\n",
    "        self.driver = driver\n",
    "    \n",
    "    def get_offers(self ,query) :\n",
    "        search_bar = driver.find_element(By.ID , 'job-search-bar-keywords')\n",
    "        search_bar.send_keys(query)\n",
    "        search_bar.send_keys(Keys.ENTER)\n",
    "        \n",
    "    def scroll_the_page(self):\n",
    "        \n",
    "        ## Scrolling til the end to show all the offers\n",
    "        SCROLL_PAUSE_TIME = 20\n",
    "\n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        \n",
    "    def scrape_offers(self):\n",
    "        self.scroll_the_page()\n",
    "        jobs = driver.find_elements(By.CLASS_NAME , 'job-search-card')\n",
    "        i = 0\n",
    "        for job in jobs :\n",
    "            job.click()\n",
    "            time.sleep(5)\n",
    "            results = self.scrape_one_offer()   \n",
    "            self.save_offers(results)\n",
    "            i+= 1\n",
    "        return i\n",
    "        \n",
    "        \n",
    "    def scrape_one_offer(self) :\n",
    "        try:\n",
    "            title = self.driver.find_element(By.CLASS_NAME , 'top-card-layout__title').text\n",
    "        except NoSuchElementException :\n",
    "            self.driver.back()\n",
    "            time.sleep(3)\n",
    "        \n",
    "        try :\n",
    "            employer = self.driver.find_element(By.CLASS_NAME , 'topcard__org-name-link').text\n",
    "        except NoSuchElementException :\n",
    "            employer = self.driver.find_elements(By.TAG_NAME , 'span')[0].text\n",
    "            \n",
    "        location = self.driver.find_element(By.CLASS_NAME ,'topcard__flavor--bullet').text\n",
    "\n",
    "        try:\n",
    "            more_button =  self.driver.find_element(By.CLASS_NAME , 'show-more-less-button')\n",
    "            more_button.click()\n",
    "\n",
    "        except NoSuchElementException :\n",
    "            pass\n",
    "\n",
    "        description = self.driver.find_element(By.CLASS_NAME , 'show-more-less-html__markup ').text\n",
    "        employer_rating = None\n",
    "        driver.back()\n",
    "        \n",
    "        job_infos = [title,employer,employer_rating,location,job_description]\n",
    "            \n",
    "        return job_infos\n",
    "    \n",
    "    def save_offers(self,infos):\n",
    "        \n",
    "        with open('Jobs.csv','a') as f :\n",
    "            writer = csv.writer(f , delimiter =';')\n",
    "            writer.writerow(job_infos)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = LinKedIn_scrapper(driver)\n",
    "scraper.get_offers(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = scraper.scrape_offers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params :\n",
    "        job = 'Data Scientist'\n",
    "        location = 'Casablanca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Scientist'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Params.job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
