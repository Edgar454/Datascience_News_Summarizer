Title;Author;Details;Post_date;URL
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;2024-03-12;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;2024-03-12;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;2024-03-12;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;2024-03-12;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;2024-03-12;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;16 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;16 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;16 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;16 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;16 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;16 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;16 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;16 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;16 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;16 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;16 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;16 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;16 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;16 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;16 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;16 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;16 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;16 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;16 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;16 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;17 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;17 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;17 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;17 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;17 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;20 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;20 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;20 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;20 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;20 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;20 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;20 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;20 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;20 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;20 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;20 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;20 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;20 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;20 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;20 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;20 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;20 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;20 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;20 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;20 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
🤗 PEFT welcomes new merging methods;Fabio Chiusano;🤗 PEFT welcomes new merging methods. Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.;Feb 19, 2024;https://huggingface.co/blog/peft_merging
Machine Learning in Chemistry;Fabio Chiusano;Machine Learning in Chemistry. Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026
Neural network training makes beautiful fractals;Fabio Chiusano;Neural network training makes beautiful fractals. Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.;Feb 19, 2024;https://sohl-dickstein.github.io/2024/02/12/fractal.html
Learning the importance of training data under concept drift;Fabio Chiusano;Learning the importance of training data under concept drift. Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.;Feb 19, 2024;https://blog.research.google/2024/02/learning-importance-of-training-data.html
GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data;Fabio Chiusano;GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data. GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
Thinking about High-Quality Human Data;Fabio Chiusano;Thinking about High-Quality Human Data. High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the “wisdom of the crowd” with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.;Feb 12, 2024;https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates;Fabio Chiusano;NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates. The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment’s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs’ reasoning performance.;Feb 12, 2024;https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval
SegMoE: Segmind Mixture of Diffusion Experts;Fabio Chiusano;SegMoE: Segmind Mixture of Diffusion Experts. SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.;Feb 12, 2024;https://huggingface.co/blog/segmoe
AI Aesthetics;Fabio Chiusano;AI Aesthetics. AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills — akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like “Dream Machine.”;Feb 12, 2024;https://www.usv.com/writing/2024/02/ai-aesthetics/
Why I Switched From Google Search to Perplexity;Fabio Chiusano;Why I Switched From Google Search to Perplexity. The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.;Feb 12, 2024;https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
Thinking about High-Quality Human Data;Fabio Chiusano;Thinking about High-Quality Human Data. High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the “wisdom of the crowd” with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.;Feb 12, 2024;https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates;Fabio Chiusano;NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates. The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment’s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs’ reasoning performance.;Feb 12, 2024;https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval
SegMoE: Segmind Mixture of Diffusion Experts;Fabio Chiusano;SegMoE: Segmind Mixture of Diffusion Experts. SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.;Feb 12, 2024;https://huggingface.co/blog/segmoe
AI Aesthetics;Fabio Chiusano;AI Aesthetics. AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills — akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like “Dream Machine.”;Feb 12, 2024;https://www.usv.com/writing/2024/02/ai-aesthetics/
Why I Switched From Google Search to Perplexity;Fabio Chiusano;Why I Switched From Google Search to Perplexity. The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.;Feb 12, 2024;https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Towards Long Context RAG;Fabio Chiusano;Towards Long Context RAG. Google’s Gemini 1.5 Pro has introduced an impressive 1-million-word context window, fueling a debate in the AI community about the future relevance of Retrieval-Augmented Generation (RAG). Anticipated advancements in RAG architectures include more efficient document retrieval techniques, better routing for reduced latency and cost, and improved key-value (KV) caching, aiming to maximize the benefits of long-context models.;Mar 4, 2024;https://www.llamaindex.ai/blog/towards-long-context-rag
TTS Arena: Benchmarking Text-to-Speech Models in the Wild;Fabio Chiusano;TTS Arena: Benchmarking Text-to-Speech Models in the Wild. TTS Arena, mirroring the concept of LMSys Chatbot Arena, provides a platform for comparing and evaluating text-to-speech models, enabling users to sample, review, and rate various systems to determine the most realistic voices.;Mar 4, 2024;https://huggingface.co/blog/arena-tts
Prompt Engineering with Llama 2;Fabio Chiusano;Prompt Engineering with Llama 2. Deeplearning.ai has launched a new course titled “Prompt Engineering with Llama 2,” designed to enhance skills in prompt creation and model optimization. The course covers advanced prompt strategies, including few-shot and chain-of-thought techniques. It also introduces Code Llama, a virtual aid for pair programming.;Mar 4, 2024;https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/
Introduction to Gorilla LLM;Fabio Chiusano;Introduction to Gorilla LLM. The Berkeley Function-Calling Leaderboard (BFCL) has established a new benchmark for evaluating LLMs on their ability to execute various types of function calls. GPT-4 is currently the top performer in function-calling tasks.;Mar 4, 2024;https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
Evaluating Quantized Large Language Models;Fabio Chiusano;Evaluating Quantized Large Language Models. This paper/guide explores the impact of post-training quantization (PTQ) on reducing memory and computational demands of large language models. It offers an extensive evaluation of PTQ’s effect on various components like Weight, Activation, and KV Cache across 11 LLM families with parameter sizes from 125 million to 180 billion.;Mar 4, 2024;https://arxiv.org/abs/2402.18158v1
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
🤗 PEFT welcomes new merging methods;Fabio Chiusano;🤗 PEFT welcomes new merging methods. Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.;Feb 19, 2024;https://huggingface.co/blog/peft_merging
Machine Learning in Chemistry;Fabio Chiusano;Machine Learning in Chemistry. Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026
Neural network training makes beautiful fractals;Fabio Chiusano;Neural network training makes beautiful fractals. Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.;Feb 19, 2024;https://sohl-dickstein.github.io/2024/02/12/fractal.html
Learning the importance of training data under concept drift;Fabio Chiusano;Learning the importance of training data under concept drift. Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.;Feb 19, 2024;https://blog.research.google/2024/02/learning-importance-of-training-data.html
GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data;Fabio Chiusano;GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data. GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
🤗 PEFT welcomes new merging methods;Fabio Chiusano;🤗 PEFT welcomes new merging methods. Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.;Feb 19, 2024;https://huggingface.co/blog/peft_merging
Machine Learning in Chemistry;Fabio Chiusano;Machine Learning in Chemistry. Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026
Neural network training makes beautiful fractals;Fabio Chiusano;Neural network training makes beautiful fractals. Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.;Feb 19, 2024;https://sohl-dickstein.github.io/2024/02/12/fractal.html
Learning the importance of training data under concept drift;Fabio Chiusano;Learning the importance of training data under concept drift. Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.;Feb 19, 2024;https://blog.research.google/2024/02/learning-importance-of-training-data.html
GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data;Fabio Chiusano;GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data. GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc
Thinking about High-Quality Human Data;Fabio Chiusano;Thinking about High-Quality Human Data. High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the “wisdom of the crowd” with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.;Feb 12, 2024;https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates;Fabio Chiusano;NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates. The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment’s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs’ reasoning performance.;Feb 12, 2024;https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval
SegMoE: Segmind Mixture of Diffusion Experts;Fabio Chiusano;SegMoE: Segmind Mixture of Diffusion Experts. SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.;Feb 12, 2024;https://huggingface.co/blog/segmoe
AI Aesthetics;Fabio Chiusano;AI Aesthetics. AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills — akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like “Dream Machine.”;Feb 12, 2024;https://www.usv.com/writing/2024/02/ai-aesthetics/
Why I Switched From Google Search to Perplexity;Fabio Chiusano;Why I Switched From Google Search to Perplexity. The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.;Feb 12, 2024;https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
🤗 PEFT welcomes new merging methods;Fabio Chiusano;🤗 PEFT welcomes new merging methods. Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.;Feb 19, 2024;https://huggingface.co/blog/peft_merging
Machine Learning in Chemistry;Fabio Chiusano;Machine Learning in Chemistry. Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026
Neural network training makes beautiful fractals;Fabio Chiusano;Neural network training makes beautiful fractals. Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.;Feb 19, 2024;https://sohl-dickstein.github.io/2024/02/12/fractal.html
Learning the importance of training data under concept drift;Fabio Chiusano;Learning the importance of training data under concept drift. Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.;Feb 19, 2024;https://blog.research.google/2024/02/learning-importance-of-training-data.html
GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data;Fabio Chiusano;GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data. GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc
Thinking about High-Quality Human Data;Fabio Chiusano;Thinking about High-Quality Human Data. High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the “wisdom of the crowd” with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.;Feb 12, 2024;https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates;Fabio Chiusano;NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates. The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment’s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs’ reasoning performance.;Feb 12, 2024;https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval
SegMoE: Segmind Mixture of Diffusion Experts;Fabio Chiusano;SegMoE: Segmind Mixture of Diffusion Experts. SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.;Feb 12, 2024;https://huggingface.co/blog/segmoe
AI Aesthetics;Fabio Chiusano;AI Aesthetics. AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills — akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like “Dream Machine.”;Feb 12, 2024;https://www.usv.com/writing/2024/02/ai-aesthetics/
Why I Switched From Google Search to Perplexity;Fabio Chiusano;Why I Switched From Google Search to Perplexity. The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.;Feb 12, 2024;https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/
Towards Long Context RAG;Fabio Chiusano;Towards Long Context RAG. Google’s Gemini 1.5 Pro has introduced an impressive 1-million-word context window, fueling a debate in the AI community about the future relevance of Retrieval-Augmented Generation (RAG). Anticipated advancements in RAG architectures include more efficient document retrieval techniques, better routing for reduced latency and cost, and improved key-value (KV) caching, aiming to maximize the benefits of long-context models.;Mar 4, 2024;https://www.llamaindex.ai/blog/towards-long-context-rag
TTS Arena: Benchmarking Text-to-Speech Models in the Wild;Fabio Chiusano;TTS Arena: Benchmarking Text-to-Speech Models in the Wild. TTS Arena, mirroring the concept of LMSys Chatbot Arena, provides a platform for comparing and evaluating text-to-speech models, enabling users to sample, review, and rate various systems to determine the most realistic voices.;Mar 4, 2024;https://huggingface.co/blog/arena-tts
Prompt Engineering with Llama 2;Fabio Chiusano;Prompt Engineering with Llama 2. Deeplearning.ai has launched a new course titled “Prompt Engineering with Llama 2,” designed to enhance skills in prompt creation and model optimization. The course covers advanced prompt strategies, including few-shot and chain-of-thought techniques. It also introduces Code Llama, a virtual aid for pair programming.;Mar 4, 2024;https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/
Introduction to Gorilla LLM;Fabio Chiusano;Introduction to Gorilla LLM. The Berkeley Function-Calling Leaderboard (BFCL) has established a new benchmark for evaluating LLMs on their ability to execute various types of function calls. GPT-4 is currently the top performer in function-calling tasks.;Mar 4, 2024;https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
Evaluating Quantized Large Language Models;Fabio Chiusano;Evaluating Quantized Large Language Models. This paper/guide explores the impact of post-training quantization (PTQ) on reducing memory and computational demands of large language models. It offers an extensive evaluation of PTQ’s effect on various components like Weight, Activation, and KV Cache across 11 LLM families with parameter sizes from 125 million to 180 billion.;Mar 4, 2024;https://arxiv.org/abs/2402.18158v1
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
Towards Long Context RAG;Fabio Chiusano;Towards Long Context RAG. Google’s Gemini 1.5 Pro has introduced an impressive 1-million-word context window, fueling a debate in the AI community about the future relevance of Retrieval-Augmented Generation (RAG). Anticipated advancements in RAG architectures include more efficient document retrieval techniques, better routing for reduced latency and cost, and improved key-value (KV) caching, aiming to maximize the benefits of long-context models.;Mar 4, 2024;https://www.llamaindex.ai/blog/towards-long-context-rag
TTS Arena: Benchmarking Text-to-Speech Models in the Wild;Fabio Chiusano;TTS Arena: Benchmarking Text-to-Speech Models in the Wild. TTS Arena, mirroring the concept of LMSys Chatbot Arena, provides a platform for comparing and evaluating text-to-speech models, enabling users to sample, review, and rate various systems to determine the most realistic voices.;Mar 4, 2024;https://huggingface.co/blog/arena-tts
Prompt Engineering with Llama 2;Fabio Chiusano;Prompt Engineering with Llama 2. Deeplearning.ai has launched a new course titled “Prompt Engineering with Llama 2,” designed to enhance skills in prompt creation and model optimization. The course covers advanced prompt strategies, including few-shot and chain-of-thought techniques. It also introduces Code Llama, a virtual aid for pair programming.;Mar 4, 2024;https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/
Introduction to Gorilla LLM;Fabio Chiusano;Introduction to Gorilla LLM. The Berkeley Function-Calling Leaderboard (BFCL) has established a new benchmark for evaluating LLMs on their ability to execute various types of function calls. GPT-4 is currently the top performer in function-calling tasks.;Mar 4, 2024;https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
Evaluating Quantized Large Language Models;Fabio Chiusano;Evaluating Quantized Large Language Models. This paper/guide explores the impact of post-training quantization (PTQ) on reducing memory and computational demands of large language models. It offers an extensive evaluation of PTQ’s effect on various components like Weight, Activation, and KV Cache across 11 LLM families with parameter sizes from 125 million to 180 billion.;Mar 4, 2024;https://arxiv.org/abs/2402.18158v1
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Towards Long Context RAG;Fabio Chiusano;Towards Long Context RAG. Google’s Gemini 1.5 Pro has introduced an impressive 1-million-word context window, fueling a debate in the AI community about the future relevance of Retrieval-Augmented Generation (RAG). Anticipated advancements in RAG architectures include more efficient document retrieval techniques, better routing for reduced latency and cost, and improved key-value (KV) caching, aiming to maximize the benefits of long-context models.;Mar 4, 2024;https://www.llamaindex.ai/blog/towards-long-context-rag
TTS Arena: Benchmarking Text-to-Speech Models in the Wild;Fabio Chiusano;TTS Arena: Benchmarking Text-to-Speech Models in the Wild. TTS Arena, mirroring the concept of LMSys Chatbot Arena, provides a platform for comparing and evaluating text-to-speech models, enabling users to sample, review, and rate various systems to determine the most realistic voices.;Mar 4, 2024;https://huggingface.co/blog/arena-tts
Prompt Engineering with Llama 2;Fabio Chiusano;Prompt Engineering with Llama 2. Deeplearning.ai has launched a new course titled “Prompt Engineering with Llama 2,” designed to enhance skills in prompt creation and model optimization. The course covers advanced prompt strategies, including few-shot and chain-of-thought techniques. It also introduces Code Llama, a virtual aid for pair programming.;Mar 4, 2024;https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/
Introduction to Gorilla LLM;Fabio Chiusano;Introduction to Gorilla LLM. The Berkeley Function-Calling Leaderboard (BFCL) has established a new benchmark for evaluating LLMs on their ability to execute various types of function calls. GPT-4 is currently the top performer in function-calling tasks.;Mar 4, 2024;https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
Evaluating Quantized Large Language Models;Fabio Chiusano;Evaluating Quantized Large Language Models. This paper/guide explores the impact of post-training quantization (PTQ) on reducing memory and computational demands of large language models. It offers an extensive evaluation of PTQ’s effect on various components like Weight, Activation, and KV Cache across 11 LLM families with parameter sizes from 125 million to 180 billion.;Mar 4, 2024;https://arxiv.org/abs/2402.18158v1
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
🤗 PEFT welcomes new merging methods;Fabio Chiusano;🤗 PEFT welcomes new merging methods. Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.;Feb 19, 2024;https://huggingface.co/blog/peft_merging
Machine Learning in Chemistry;Fabio Chiusano;Machine Learning in Chemistry. Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026
Neural network training makes beautiful fractals;Fabio Chiusano;Neural network training makes beautiful fractals. Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.;Feb 19, 2024;https://sohl-dickstein.github.io/2024/02/12/fractal.html
Learning the importance of training data under concept drift;Fabio Chiusano;Learning the importance of training data under concept drift. Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.;Feb 19, 2024;https://blog.research.google/2024/02/learning-importance-of-training-data.html
GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data;Fabio Chiusano;GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data. GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc
Thinking about High-Quality Human Data;Fabio Chiusano;Thinking about High-Quality Human Data. High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the “wisdom of the crowd” with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.;Feb 12, 2024;https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates;Fabio Chiusano;NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates. The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment’s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs’ reasoning performance.;Feb 12, 2024;https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval
SegMoE: Segmind Mixture of Diffusion Experts;Fabio Chiusano;SegMoE: Segmind Mixture of Diffusion Experts. SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.;Feb 12, 2024;https://huggingface.co/blog/segmoe
AI Aesthetics;Fabio Chiusano;AI Aesthetics. AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills — akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like “Dream Machine.”;Feb 12, 2024;https://www.usv.com/writing/2024/02/ai-aesthetics/
Why I Switched From Google Search to Perplexity;Fabio Chiusano;Why I Switched From Google Search to Perplexity. The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.;Feb 12, 2024;https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;21 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;21 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;21 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;21 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;21 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Towards Long Context RAG;Fabio Chiusano;Towards Long Context RAG. Google’s Gemini 1.5 Pro has introduced an impressive 1-million-word context window, fueling a debate in the AI community about the future relevance of Retrieval-Augmented Generation (RAG). Anticipated advancements in RAG architectures include more efficient document retrieval techniques, better routing for reduced latency and cost, and improved key-value (KV) caching, aiming to maximize the benefits of long-context models.;Mar 4, 2024;https://www.llamaindex.ai/blog/towards-long-context-rag
TTS Arena: Benchmarking Text-to-Speech Models in the Wild;Fabio Chiusano;TTS Arena: Benchmarking Text-to-Speech Models in the Wild. TTS Arena, mirroring the concept of LMSys Chatbot Arena, provides a platform for comparing and evaluating text-to-speech models, enabling users to sample, review, and rate various systems to determine the most realistic voices.;Mar 4, 2024;https://huggingface.co/blog/arena-tts
Prompt Engineering with Llama 2;Fabio Chiusano;Prompt Engineering with Llama 2. Deeplearning.ai has launched a new course titled “Prompt Engineering with Llama 2,” designed to enhance skills in prompt creation and model optimization. The course covers advanced prompt strategies, including few-shot and chain-of-thought techniques. It also introduces Code Llama, a virtual aid for pair programming.;Mar 4, 2024;https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/
Introduction to Gorilla LLM;Fabio Chiusano;Introduction to Gorilla LLM. The Berkeley Function-Calling Leaderboard (BFCL) has established a new benchmark for evaluating LLMs on their ability to execute various types of function calls. GPT-4 is currently the top performer in function-calling tasks.;Mar 4, 2024;https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
Evaluating Quantized Large Language Models;Fabio Chiusano;Evaluating Quantized Large Language Models. This paper/guide explores the impact of post-training quantization (PTQ) on reducing memory and computational demands of large language models. It offers an extensive evaluation of PTQ’s effect on various components like Weight, Activation, and KV Cache across 11 LLM families with parameter sizes from 125 million to 180 billion.;Mar 4, 2024;https://arxiv.org/abs/2402.18158v1
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
🤗 PEFT welcomes new merging methods;Fabio Chiusano;🤗 PEFT welcomes new merging methods. Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.;Feb 19, 2024;https://huggingface.co/blog/peft_merging
Machine Learning in Chemistry;Fabio Chiusano;Machine Learning in Chemistry. Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026
Neural network training makes beautiful fractals;Fabio Chiusano;Neural network training makes beautiful fractals. Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.;Feb 19, 2024;https://sohl-dickstein.github.io/2024/02/12/fractal.html
Learning the importance of training data under concept drift;Fabio Chiusano;Learning the importance of training data under concept drift. Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.;Feb 19, 2024;https://blog.research.google/2024/02/learning-importance-of-training-data.html
GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data;Fabio Chiusano;GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data. GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc
Thinking about High-Quality Human Data;Fabio Chiusano;Thinking about High-Quality Human Data. High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the “wisdom of the crowd” with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.;Feb 12, 2024;https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates;Fabio Chiusano;NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates. The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment’s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs’ reasoning performance.;Feb 12, 2024;https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval
SegMoE: Segmind Mixture of Diffusion Experts;Fabio Chiusano;SegMoE: Segmind Mixture of Diffusion Experts. SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.;Feb 12, 2024;https://huggingface.co/blog/segmoe
AI Aesthetics;Fabio Chiusano;AI Aesthetics. AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills — akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like “Dream Machine.”;Feb 12, 2024;https://www.usv.com/writing/2024/02/ai-aesthetics/
Why I Switched From Google Search to Perplexity;Fabio Chiusano;Why I Switched From Google Search to Perplexity. The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.;Feb 12, 2024;https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/
A Practical Guide to RAG Pipeline Evaluation (part 1);Fabio Chiusano;A Practical Guide to RAG Pipeline Evaluation (part 1). An analysis of LLMs such as GPT-4 in the context of retrieval systems shows that while they decently determine context relevance with a 79% accuracy rate for binary relevance, there are challenges faced in terms of low recall and dealing with multiple relevant contexts in complicated queries, indicating room for improvement in precision and recall metrics.;22 hours ago;https://medium.com/relari/a-practical-guide-to-rag-pipeline-evaluation-part-1-27a472b09893
Training great LLMs entirely from ground up in the wilderness as a startup;Fabio Chiusano;Training great LLMs entirely from ground up in the wilderness as a startup. In the AI startup domain, the process of training large language models hinges not merely on expertise but also on the careful selection of hardware infrastructure. Subpar or inconsistent GPU performance due to quality differences in clusters can significantly hinder model training efficacy.;22 hours ago;https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
Gemma on Android and iPhone and more local LLM updates from MLC;Fabio Chiusano;Gemma on Android and iPhone and more local LLM updates from MLC. The Gemma2B language model can be used on mobile platforms, including Android and iPhone, featuring offline functionality. Leveraging SLM compilation by MLC for Python, the 2-billion parameter model achieves a generation speed of 20 tokens per second on devices as efficient as the Samsung S23 without requiring an internet connection. Enhanced optimization is achieved through model quantization.;22 hours ago;https://medium.com/octoml/look-ma-no-wifi-gemma-on-android-and-iphone-and-more-local-llm-updates-from-mlc-239f10adba3c
You can now train a 70b language model at home;Fabio Chiusano;You can now train a 70b language model at home. Answer.ai is introducing an open source system leveraging FSDP and QLoRA that enables the training of a 70 billion parameter language model on just two 24GB GPUs.;22 hours ago;https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html
Captain’s log: the irreducible weirdness of prompting AIs;Fabio Chiusano;Captain’s log: the irreducible weirdness of prompting AIs. Effective prompting techniques such as adding rich contexts, custom examples, and adopting a “Chain of Thought” strategy significantly enhance the performance of AI models like Meta’s Llama 2 or GPT-4.;22 hours ago;https://www.oneusefulthing.org/p/captains-log-the-irreducible-weirdness
Top 11 AI-Powered Image Generators in 2024;Fabio Chiusano;Top 11 AI-Powered Image Generators in 2024. AI-powered image generators such as DALLE 3, Midjourney, Dream Studio (Stable Diffusion), Canva AI, and NightCafe are transforming the landscape of visual creation, offering innovative tools for professional design, business applications, and art enthusiasts.;Feb 26, 2024;https://pub.towardsai.net/top-11-ai-powered-image-generators-in-2024-c6a06e9697e8
Build an LLM-Powered Data Agent for Data Analysis;Fabio Chiusano;Build an LLM-Powered Data Agent for Data Analysis. This guide outlines the necessary agent types and their collaborative roles in creating a proficient LLM application for data analysis tasks. It includes a practical use case and corresponding code snippets, alongside optimization tips for AI developers involved in the design and implementation of LLM agent applications.;Feb 26, 2024;https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/
Advanced Techniques for Research with ChatGPT;Fabio Chiusano;Advanced Techniques for Research with ChatGPT. This guide outlines strategies for leveraging ChatGPT in research, emphasizing that while ChatGPT can streamline research tasks, the quality of research still depends on the human researcher’s expertise and understanding.;Feb 26, 2024;https://machinelearningmastery.com/advanced-techniques-for-research-with-chatgpt/
How many news websites block AI crawlers?;Fabio Chiusano;How many news websites block AI crawlers?. U.S. news publishers are increasingly blocking AI crawlers from companies such as OpenAI and Google, with 80% of top U.S. sites restricting OpenAI’s access as of late 2023. The trend exhibits significant variation internationally, with only 20% of leading news sites in Mexico and Poland implementing similar blocks.;Feb 26, 2024;https://reutersinstitute.politics.ox.ac.uk/how-many-news-websites-block-ai-crawlers
My benchmark for large language models;Fabio Chiusano;My benchmark for large language models. This benchmark assesses the capabilities of large language models in real-world programming tasks, such as code translation between Python and C, understanding minified JavaScript, and generating SQL from English.;Feb 26, 2024;https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html
🤗 PEFT welcomes new merging methods;Fabio Chiusano;🤗 PEFT welcomes new merging methods. Model merging has quickly become a de-facto standard of pushing the performance limits of large language models. On the Open LLM Leaderboard, there are new merged models topping up the charts.;Feb 19, 2024;https://huggingface.co/blog/peft_merging
Machine Learning in Chemistry;Fabio Chiusano;Machine Learning in Chemistry. Machine learning and neural networks like CNNs and RNNs are significantly advancing chemical research by identifying patterns in complex data, aiding in drug development, toxicity prediction, and understanding structure-activity relationships.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/machine-learning-in-chemistry-87e6ff866026
Neural network training makes beautiful fractals;Fabio Chiusano;Neural network training makes beautiful fractals. Neural network training can inadvertently generate intricate fractals, reflecting the dynamic interplay of hyperparameter settings, particularly the learning rate. As the setting of the learning rate is fine-tuned to avoid divergence and ensure efficient training, the boundary between effective training and failure manifests as a fractal pattern.;Feb 19, 2024;https://sohl-dickstein.github.io/2024/02/12/fractal.html
Learning the importance of training data under concept drift;Fabio Chiusano;Learning the importance of training data under concept drift. Recent research proposes a relevance-based ranking system for training data, utilizing a helper model designed to address and adapt to slow concept drift in AI. This approach has the potential to enhance model performance over time, presenting a competitive solution for improved adaptability in continual learning.;Feb 19, 2024;https://blog.research.google/2024/02/learning-importance-of-training-data.html
GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data;Fabio Chiusano;GeoGPT+ Tutorial: Web-Ready Map Visuals From GIS Forest Fire Data. GeoGPT+ is a geospatial GPT tool designed for real-time data integration and visual map generation from GIS datasets. Notably, it specializes in interpreting forest fire data from sources like the NASA Forest Fires CSV dataset, transforming raw data into meaningful visuals.;Feb 19, 2024;https://medium.com/towards-artificial-intelligence/geogpt-tutorial-web-ready-map-visuals-from-gis-forest-fire-data-f9ecb7e214dc
Thinking about High-Quality Human Data;Fabio Chiusano;Thinking about High-Quality Human Data. High-quality, detailed human annotations are crucial for creating effective deep learning models, ensuring AI accuracy through tasks such as content classification and language model alignment. Leveraging the “wisdom of the crowd” with appropriate filters can produce results comparable to expert quality, as proven by research in areas like machine translation.;Feb 12, 2024;https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates;Fabio Chiusano;NPHardEval Leaderboard: Unveiling the Reasoning Abilities of Large Language Models through Complexity Classes and Dynamic Updates. The NPHardEval leaderboard offers a benchmark for evaluating the reasoning capabilities of LLMs on a set of 900 algorithmic problems, focusing on NP-Hard and less complex tasks. To maintain the assessment’s integrity and prevent model overfitting, it undergoes monthly updates with new challenges, which helps to reliably measure the progress in LLMs’ reasoning performance.;Feb 12, 2024;https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval
SegMoE: Segmind Mixture of Diffusion Experts;Fabio Chiusano;SegMoE: Segmind Mixture of Diffusion Experts. SegMoE, embedded within the Hugging Face ecosystem, simplifies the development of bespoke Mixture- of-Experts Diffusion models influenced by Stable Diffusion, incorporating sparse MoE layers for specific token processing.;Feb 12, 2024;https://huggingface.co/blog/segmoe
AI Aesthetics;Fabio Chiusano;AI Aesthetics. AI is transforming the art landscape by facilitating the creation of visuals, music, and narratives, making it accessible to individuals regardless of their artistic skills — akin to how smartphones popularized photography. It has enabled the emergence of new genres and interactive art that turns consumers into collaborators, utilizing tools such as storytelling AI chatbots and dynamic installations like “Dream Machine.”;Feb 12, 2024;https://www.usv.com/writing/2024/02/ai-aesthetics/
Why I Switched From Google Search to Perplexity;Fabio Chiusano;Why I Switched From Google Search to Perplexity. The author discusses transitioning from Google to the AI-powered search engine Perplexity, highlighting its key features like delivering summarized, direct answers with source citations, and the unique ability to provide rewritten perspectives.;Feb 12, 2024;https://shellypalmer.com/2024/02/why-i-switched-from-google-search-to-perplexity/
